\section{Handwritten digit denoising}
The learn more about the two techniques, both will be applied to a digit recognition problem using the supplied script \texttt{digitsdn}. The original data set consists of handwritten numbers with approximately 20 records per number from $0$ to $9$. Each image has a resolution of $16$ by $15$. Two test sets with one image per numeral are also included. The images are flattened into row vectors and stored in an input matrix with one image The top rows of the plots in figure~\ref{fig:denoise2} show the noise free data of the first test set. The second row noisy versions of the same hand written numbers. 
In the non-linear case the rbf-kernel-function density is initialized using the approximation dimension times the mean of the variance in the training set. Figure~\ref{fig:pcKerLin} shows the principal components found by using linear and kernel pca on the large training data set. These principal components will be used later to clean the noisy numerals in the test set. The plots reveal that a seven shaped principal component is dominant in the kernel as well as the linear case. All rows after the second row in figure~\ref{fig:denoise2} show efforts to reconstruct de-noised versions of the input using an increasing number of principal components. If few components are used the dominant seven shaped vector takes over the reconstruction quite often. If a more complete model with more components is used this does not happen anymore in the linear as well as the nonlinear case. The crucial difference between the two cases is that in the linear case the noise reduction decreases when more principal components are used, resembling the original image more and more. In the kernel case noise reduction gets better each time moving more towards a prototypical representation of the input. This behavior probably stems from the fact that the kernel case is able to adapt better to non-linear number shapes. 
\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{../src/figure/pcKerLin}
\caption{The principal components using linear PCA (first row). Kernel PCA principal components (second row) and projections using the first principal components (third row).}
\label{fig:pcKerLin}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=0.49\linewidth]{../src/figure/denoise1}
\includegraphics[width=0.49\linewidth]{../src/figure/denoise2}
\caption{Digit de-noising using kernel (left) and linear PCA (right).}
\label{fig:denoise2}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=0.49\linewidth]{../src/figure/denoiseKerLargSigm}
\caption{The kernel pca process using an rbf function with larger width $\sigma^2$.}
\label{fig:denoiseKerLargSigm}
\end{figure}
The miss-representation of the number seven in the can be fixed by increasing the rbf-kernel width, the result is shown in figure~\ref{fig:denoiseKerLargSigm}. Unfortunately increasing the width also increases the noise, so a good trade off is required. 

\section{Spectral clustering}
In this section a problem consisting of two interlocked rings as shown in figure~\ref{fig:interlock} will be analyzed. Two the human eye the grouping of points into two independent rings is immediately apparent. Spectral clustering is an algorithm, with which a computer can find these two classes. As the data set does not have annotations, this is an unsupervised learning algorithm. Spectral clustering methods use a kernel function as a similarity measure. The largest eigenvectors of a rescaled kernel-matrix are computed. The eigenvectors corresponding to the three largest eigenvalues are computed the second largest will contain the clustering information in the sign. If more then three eigenvectors are found the one containing the clustering information will be the smallest with an even number, if one starts counting with one at the largest vector. It must be noted that in this case the eigenvalues are all very close to one, so this observation cannot necessarily be generalized.  Figure~\ref{fig:clusteringResults} shows the results of using classification information from the second largest eigenvector when computing the first three eigenvectors using lanczos' iterative method. It can be observed that for the two smaller values $\sigma^2 = 0.001$ and $\sigma^2 = 0.01$ the method is able classify the two rings correctly. For the larger value $\sigma^2 = 0.1$ the rbf-kernel becomes to large, which results in incorrect linear looking classification.  

\begin{figure}
\centering
\input{../src/tikz/spectralClustering.tex}
\caption{The training data set consisting of two interlocked rings.}
\label{fig:interlock}
\end{figure}



\begin{figure}
\centering
\input{../src/tikz/spectralClusteringResult0.001.tex}
\input{../src/tikz/spectralClusteringSpace0.001}
\caption{Spectral clustering and subspace projections using $\sigma^2 = 0.001$.}
\input{../src/tikz/spectralClusteringResult0.01.tex}
\input{../src/tikz/spectralClusteringSpace0.01}
\caption{Spectral clustering and subspace projections using $\sigma^2 = 0.01$.}
\input{../src/tikz/spectralClusteringResult0.1.tex}
\input{../src/tikz/spectralClusteringSpace0.1}
\caption{Spectral clustering and subspace projections using $\sigma^2 = 0.1$.}
\label{fig:clusteringResults}
\end{figure}


