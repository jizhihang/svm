\section{Geometric construction of a classifier}
As a first experiment a classifier will be constructed geometrically. This is done by computing the average sample for each of the two training Gaussian distributed data sets under consideration. Next the midpoint of the two average points is found. The classification boundary is drawn with at a $\pi/2$ angle to the line connecting the two average points. The result is shown in figure~\ref{fig:initGauss} on the left. A couple of points are misclassified by this approach, which given the statistical nature of the problem must always be allowed. In this case this method produces a decent classifier, this must not be true for other distributions though. If for example many additional samples distributed as shown in figure~\ref{fig:initGauss} on the right are added the method breaks down. This happened because the new samples moved the average and with it the decision boundary. In this new situation a large portion of the blue set is misclassified. 
Points far from the decision boundary should not influence the classifier like this. This does not happen to this extend if an optimal margin is sought which maximizes the distance of the classifier to each data set if separable or the best possible separation in terms of classification if the data set is not. Figure~\ref{fig:initGaussSvm} illustrates this.
\begin{figure}
\input{../src/tikz/initialGauss.tex}
\input{../src/tikz/initialGaussMod.tex}
\caption{Geometrical construction of a linear classification line using average values.}
\label{fig:initGauss}
\end{figure}
\begin{figure}
\input{../src/tikz/initialGaussSvm.tex}
\input{../src/tikz/initialGaussModSvm.tex}
\caption{Geometrical construction of a linear classification line using average values.}
\label{fig:initGaussSvm}
\end{figure}

\section{Vapnik Support Vector machines}
At the hart of vapnik's theory is the optimal hyperplane algorithm. In the linear the hyperplane condition is given as\footnote{Support-Vector Networks
CORINNA CORTES VLADIMIR VAPNIK,Machine Leaming, 20, (1995) page 291 or Least Squares Support Vector Machines, Suykens et al., page 31.}
\begin{equation}
y_k [\mathbf{w}^T \mathbf{x}_k + b] \geq 1, \;\; k = 1,\dots,N
\end{equation}
With $x_k$ being the input data points and $y_k$ the desired output. Training the machine means finding the high dimensional hyperplane normal vector $w$ and the bias scalar $b$. Normal means here that the dot product with any vector lying in the plane must be zero. Technically a plane is defined by $\mathbf{w}^T(\mathbf{x} - \mathbf{p}) = 0$ but its only interesting here to evaluate the classifier so $b = \mathbf{p}^T \mathbf{w}$ can be used instead and the displacement within the feature space remains unknown.
The next step is to formulate the optimization problem
\begin{equation}
\min_{w,b} \frac{1}{2} \mathbf{w}^T \mathbf{w} \;\text{ such that }\; y_k [\mathbf{w}^T \mathbf{x}_k + b] \geq 1.
\end{equation}
Formulating the Lagrange dual, and taking the gradient of $\mathcal{L}(\mathbf{w},b;\alpha)$ with respect to $(\mathbf{w},b)$ leads to a problem in $\alpha$. The lagrange multipliers are called $\alpha$, but in this context they will be called support vectors of the solution. From an optimization point of view, the support vectors are Lagrange multipliers with active set indices. A problem reformulation using a mapping to a high dimensional feature space $\varphi(\mathbf{x}): \mathbb{R}^n \rightarrow \mathbb{R}^m$ as
\begin{equation}
y_k [\mathbf{w}^T \varphi(\mathbf{x}_k) + b] \geq 1
\end{equation}
allows for different kernel options. The classifier is given by
\begin{equation}
y(\mathbf{x}) = \text{sign}(\sum^N_{k=1} \alpha_k y_k K(\mathbf{x},\mathbf{x}_k) + b).
\end{equation}
The following three kernels will be considered here\footnote{Least Squares Support Vector Machines, Suykens et al., page 43.}
\begin{align}
K(\mathbf{x},\mathbf{x}_k)  &= \mathbf{x}_k^T \mathbf{x} &\text{  (linear SVM)}, \\
K(\mathbf{x}, \mathbf{x}_k) &= \exp (-\| \mathbf{x} - \mathbf{x}_k \|^2_2 / \sigma^2) &\text{  (RBF Kernel)}.
\end{align}

\begin{figure}
\centering
\includegraphics[width=0.33\linewidth]{../src/figure/svmjsLinSep}
\includegraphics[width=0.33\linewidth]{../src/figure/svmjsLinRBF}
\caption{Support vector machine classification of an almost linearly separable problem using a linear and an radial basis function kernel}
\label{fig:svmjsLinRBF}
\end{figure}

If the problem is not separable, it means that classification cannot be done without error. In the underlying optimization problem slack variables have to be included in the formulation:
\begin{equation}
y_k [\mathbf{w}^T \varphi(\mathbf{x}_k) + b] \geq 1 - \xi_k
\end{equation}
Which leads to the optimization problem:
\begin{align}
\min_{w,b,\xi} J_p(w,\xi) = \frac{1}{2}\mathbf{w}^T\mathbf{w} + c \sum_{k = 1}^N \xi_k \\
\text{such that }   y_k [\mathbf{w}^T \varphi(\mathbf{x}_k) + b] \geq 1 - \xi_k, \;\; k = 1,\dots,N \\
\xi_k \geq 0, \;\;\; k = 1,\dots,N 
\end{align}
Where $c$ is the positive real miss-classification penalty constant. High values of $c$ make erroneous classification more costly in terms of the merit function.  Low values make those cheaper. Using the \texttt{svmjs}\footnote{\url{http://cs.stanford.edu/people/karpathy/svmjs/demo/}} package the result of using different values of $c$ is illustrated in figure~\ref{fig:c}. Choosing the right value for $c$ is a balancing act. It is important to choose $c$ not too small. As very small values will result in underfitting the problem. The resulting classifier will ignore important features of the problem, as illustrated on the right in figure~\ref{fig:c}. On the other hand however $c$ must also not be too large. If the penalty on incorrect classification is too large the support machine will start to memorize nosy details, as illustrated in figure~\ref{fig:c} on the left. A good choice like the one shown in the middle captures the key points while not falling prey to the noise, while using just as many support vectors as necessary. 
\begin{figure}
\centering
\includegraphics[width=0.3\linewidth]{../src/figure/overfit}
\includegraphics[width=0.3\linewidth]{../src/figure/good}
\includegraphics[width=0.3\linewidth]{../src/figure/tooLow}
\caption{The effect of a large miss-classification penalty constant $c$ (left), a good choice for $c$ (middle) and a too small penalty value (right).}
\label{fig:c}
\end{figure}
The effect of changing the kernel density $\sigma$ is explored in figure $\ref{fig:sigma}$. In this case the classification error increase significantly for very small or very large $\sigma$ values. 
\begin{figure}
\centering
\includegraphics[width=0.3\linewidth]{../src/figure/sigmaTooSml}
\includegraphics[width=0.3\linewidth]{../src/figure/sigmaGood}
\includegraphics[width=0.3\linewidth]{../src/figure/sigmaTooLarge}
\caption{Using a  too small (left), a nicely chosen (middle) and too large (right) radial basis function parameter $\sigma$. }
\label{fig:sigma}
\end{figure}
Like choosing a good $c$ value, when picking $\sigma$ under and over-fitting considerations are important. Choosing the kernel too small as shown in figure~\ref{fig:sigma} on the left will result in over-fitting. Even tough all points are classified correctly the model misses the general overall geometry of the input data points completely. If $\sigma$ is too large under-fitting is observed in this case. Results are better in comparison to the very small $\sigma$-value but one would expect to see results of similar quality from a simple linear kernel. A good choice such as the one in figure~\ref{fig:sigma} in the middle captures the big picture while not using excessive amounts of support vectors.

\begin{figure}
\centering
\includegraphics[width=0.33\linewidth]{../src/figure/largeRBF}
\includegraphics[width=0.33\linewidth]{../src/figure/twoNewPoints}
\caption{The effect of two additional points on the boundary}
\label{fig:twoNewPoints}
\end{figure}
Figure~\ref{fig:twoNewPoints} illustrates the effect new data points have on the decision boundary. In the right plot two new red points have been added inside of the formerly entirely green region. The decision boundary changes if no additional miss-classification follows from this change. For the new red point close to the green boundary this cannot be done without miss-classifying the green points at the boundary, which therefore has to remain unchanged. 

\section{Least Squares Support Vector machines}
In contrast to classical support vector machines, their least squares version is defined as\footnote{Least Squares Support Vector Machines, Suykens et al., page 72.}:
\begin{align}
\min_{w,b,e} J_p (w,e) &= \frac{1}{2}\mathbf{w}^T\mathbf{w} + \gamma \frac{1}{2} \sum_{k=1}^N e_k^2 \\
\text{such that } y_k [\mathbf{w}^T \varphi(\mathbf{x_k}) + b] &= 1 - e_k, \; k = 1,\dots,N
\end{align}
In comparison to the Vapnik formulation the error variable is squared and the constraint is turned into an equality constraint. After training the dual space classifier
\begin{equation}
y(x) = \text{sign}[\sum_{k = 1}^N \alpha_k y_k K(x,x_k) + b]
\end{equation}
is obtained. Least squares support vector machines (LSSVM) are an attempt to reduce the computational load for very large data sets. \\
In a first experiment the iris data set\footnote{\url{http://archive.ics.uci.edu/ml/datasets/Iris}} is used to test the LSSVM algorithm and learn more about the hyper-parameter space it work well in. The challenge here is to differentiate three types of iris plants \textit{Iris Setosa}, \textit{Iris Versicolour} and \textit{Iris Virginica}, based on the leaf dimensions: sepal length [\texttt{cm}], sepal width [\texttt{cm}], petal length [\texttt{cm}], petal width [\texttt{cm}].
\begin{figure}
\centering
\includegraphics[width=0.25\linewidth]{../src/figure/petalSepalUP}
\input{../src/tikz/irisPetal.tex}
\input{../src/tikz/irisSepal.tex}
\caption{Illustration of the difference between petal and sepal flower leafs. As well as an illustration of the data distribution with blue for \textit{Iris-setosa}, red indicating \textit{Iris-versicolor} and yellow \textit{Iris-virginica}.}
\label{fig:originalIris}
\end{figure}
Figure~\ref{fig:originalIris} shows what petal and sepal flower leafs are\footnote{\url{https://en.wikipedia.org/wiki/Sepal}} as well as the data points included in the original iris data set. For a first LSSVM trial run the simplified version shown in figure~\ref{fig:simpleIris} is used. 
\begin{figure}
\centering
\input{../src/tikz/irisSimple.tex}
\caption{Simplified iris data set, only petal leafs and only two species \textit{Iris-setosa-virginica} and \textit{Versicolor} are considered.}
\label{fig:simpleIris}
\end{figure}
\begin{figure}
\centering
\input{../src/tikz/irisLinerKrnl.tex}
\input{../src/tikz/irisRBFKrnl.tex}
\input{../src/tikz/irisPolyKrnl.tex}
\caption{Classification of the simplified iris data set using LSSVMs with different kernels using two classes in each case.}
\label{fig:linRBFPoly}
\end{figure}
Figure~\ref*{fig:linRBFPoly} shows the effect of three kernels on the classification results. As the merged \textit{Setosa-Virginica} data points surround the \textit{Versicolor} measurements there is no way these could be seperated by a simple line. A linear-svm is therefore bound to fail. It comes as no surprise that the linear-kernel did not deliver meaningful results, as shown in figure~\ref{fig:linRBFPoly}. The radial basis function (rbf) and the polynomial kernel are able to cope with the complexity of the problem. The next section will explore which parameter values should be chosen for both kernel types.

\subsection{RBF kernel parameter selection for the iris data set}
\begin{figure}
\centering
\input{../src/tikz/rbfLogSigmaPlot.tex}
\input{../src/tikz/rbfLogGammaPlot.tex}
\caption{Miss-classification of test samples versus $\sigma$ and $\gamma$. The test set performance is shown in blue the training set in red.}
\label{fig:rbfSigmaGamma}
\end{figure}
When using a radial basis function kernel the training process depends on the two parameters $\sigma$ and $\gamma$. It was observed earlier that too small values of $\sigma$ lead to over-fitting and too large ones will lead to an oversimplified classifier. This picture is repeated in figure~\ref{fig:rbfSigmaGamma}, on the left. The second parameter $\gamma$  is the regularization parameter. If a small value is chosen minimizing of the complexity of the model is emphasized. For a large $\gamma$ on the other hand, good fitting
of the training data points is stressed. This leads to a situation where the too large $\gamma$ values cause overfitting and to small ones oversimplification as can be seen in figure~\ref*{fig:rbfSigmaGamma}. 

\subsection{Polynomial kernel parameter selection for the iris data set}
\begin{figure}
\centering
\input{../src/tikz/polyDegreePlot.tex}
\input{../src/tikz/polyGammaPlot.tex}
\caption{Miss-classification of test samples versus polynomial degree and $\gamma$. The test set performance is shown in blue the training set in red.}
\label{fig:ployDegGamma}
\end{figure}
Polynomial kernels follow the similar pattern. Figure~\ref*{fig:ployDegGamma} reveals that when one chooses the degree of the kernel polynomial too large the risk of over-fitting is high. A degree chosen too small will not capture the problem to its full extent. Gamma behaves with polynomial kernels just like it did with rbf-kernels.  

\subsection{The impact of different validation methods}
In the previous section the importance of avoiding under- or over-fitting became clear. In this section three different validation methods will be compared. The first procedure randomly splits the one-hundred data points of the simplified iris data set into a twenty point validation and a hundred point training data set. Using the training data a svm is trained and then tested on the validation set. The measured miss-classification using this approach for $\gamma \in [10^{-10},10^{10}]$ and $\sigma \in [10^{-2},10^{2}]$ is shown in figure~\ref{fig:classicClass}. \\
Cross-validation is a methods which attempts to use a data set more efficiently. During $k$-fold-cross-validation the data set is split into $k$ parts. During the $k$ iterations the methods runs each part is set aside for validation once. A classifier is then trained using the remaining data points. The obtained model can then be tested on the validation part. During the next iteration another part will be ignored during training and so on. Cross validation has the added benefit, that each set serves as a validation set once, which hopefully gives a more complete picture of classification performance. 
Finally the leave one out method uses all data points except for one to train the classifier. The remaining data point can then be used to evaluate the classifier found. In a next iteration another point is ignored the model is retrained and so on. The leave one out methods is the most conservative in terms of validation set size, but also requires the most computational resources as many iterations are required to get a meaningful idea about the classification performance. 
Figures~\ref{fig:classicClass},~\ref{fig:crossClass} and~\ref{fig:looClass} show the validation results of the three methods using the same parameter ranges. For poor choices of $\sigma$ and $\gamma$, correct classification sinks to about fifty percent, which is as good as classification at random. Generally speaking the 80\% split method is the most data hungry, and it gives an edgy representation of svm performance. Cross validation is more efficient on the data and results are smoother but still follow a similar pattern. The leave one out methods produces an incorrect blue area in the top right for very small $\gamma$ and $\sigma$ values, which does not appear when using other methods. Generally speaking the leave one out methods is suitable for small data-sets due to its computational inefficiency. 


\begin{figure}
\centering
\input{../src/tikz/8020plot.tex}
\caption{Miss-classification of validation data using a 80\% training and 20\% validation data ratio.}
\label{fig:classicClass}
\input{../src/tikz/crossValidPlot.tex}
\caption{Miss-classification using ten fold cross-validation.}
\label{fig:crossClass}
\input{../src/tikz/leaveOnePlot.tex}
\caption{Miss-classification using leave one out validation.}
\label{fig:looClass}
\end{figure}

\subsection{Automatic tuning}
Using the ls-svm toolbox'\footnote{\url{http://www.esat.kuleuven.be/sista/lssvmlab/}} \texttt{tunesvm} function automatic hyper-parameter selection is explored. Two algorithm pairs are available. The first pair is coupled simulated annealing or randomized directional search. The optimization function can be a simplex-type method or brute force gridsearch. Finally classification cost can be evaluated using $k$-cross-validation or leave-one-out. When choosing a combination a speed or accuracy trade-off has to be made. The fastest but most unstable combination tried during experiments for this report was the randomized directional search coupled with simplex optimization and cross validation. A histogram showing the $\gamma$ and $\sigma$ values found during fifty training processes is shown in figure~\ref{fig:tuneDsSimplexCrossv}.
A instead of the randomized directional search coupled simulated annealing can be used. Which is not parallelized but the results if finds are more consistent, as can be seen in figure~\ref{fig:tuneCsaSimplexCrossv}. The most stable algorithm combination in terms of predictability was simulated annealing, grid search, leave-one-out validation combination. A histogram with tuning results is shown in figure~\ref{fig:tuneCsaGridLeave}. Only one of fifty iterations produced a large outlier. 


\begin{figure}
\centering
\input{../src/tikz/tuneDsSimplexCrossvGamma.tex}
\input{../src/tikz/tuneDsSimplexCrossvSigma.tex}
\caption{Histogram of automatically tunned  $\gamma$ and $\sigma$ values using randomized directional search, cross-validation together with simplex-optimization.}
\label{fig:tuneDsSimplexCrossv}
\end{figure}
\begin{figure}
\centering
\input{../src/tikz/tuneCsaSimplexCrossvGamma.tex}
\input{../src/tikz/tuneCsaSimplexCrossvSigma.tex}
\caption{Histogram of automatically tunned  $\gamma$ and $\sigma$ values using coupled simulated annealing, cross-validation together with simplex-optimization.}
\label{fig:tuneCsaSimplexCrossv}
\end{figure}

\begin{figure}
\centering
\input{../src/tikz/tuneCsaGridLeaveGamma.tex}
\input{../src/tikz/tuneCsaGridLeaveSigma.tex}
\caption{Histogram of automatically tunned  $\gamma$ and $\sigma$ values using coupled simulated annealing, leace-one-out-validation together with a grid search algorithm.}
\label{fig:tuneCsaGridLeave}
\end{figure}

\subsection{The roc-curve}
The receiver operating characteristic (roc)-curve is a way to asses the performance of a binary classifier as its discrimination threshold is varied. Defining\footnote{Least Squares Support Vector Machines, Suykens et al., page 19.}
\begin{align}
\text{Sensitivity } &= \frac{\text{TP}}{\text{TP} + \text{FN}} \\ 
\text{Specifity } &= \frac{\text{TN}}{\text{FP} + \text{TN}} \\
\text{False pos. rate } &= 1 - \text{Specifity} = \frac{\text{FP}}{\text{FP} + \text{TN}}
\end{align}
With TP = \textquotedblleft true positive\textquotedblright,
 FP = \textquotedblleft false positive\textquotedblright and
 TN = \textquotedblleft true negative\textquotedblright. The sensitivity is thus the ratio of correctly classified positives over the sum of true positives and false negatives, which is the total number of positive cases that should have been identified as such. Ideally one would aim for a sensitivity of one. Similarly the specificity is the number of true negatives over the total of cases that should have been classified as negative.
Going trough possible decision threshold values the roc-curve is drawn. The area under the curve describes the efficency of the classifier. If it is one, a perfect classifier has been found, such as the one in figure~\ref{fig:roc}. If the area is one half, the classifier has no added value over classification at random.  
 

\begin{figure}
\centering
\input{../src/tikz/roc.tex}
\caption{}
\label{fig:roc}
\end{figure}


\subsection{Full complexity iris data set classification}
There is no reason not to subject least squares support vector machines to the full four dimensional classification problem as seen in figure~\ref{fig:originalIris}. An experiment is done where 30 of the 150 iris plant data points are set aside for validation. The remaining values serve as training data. An svm with $\gamma = 1$ and $\sigma^2 = 1$ did miss-classify only one of 30 validation iris flowers.