\section{Geometric construction of a classifier}
As a first experiment a classifier will be constructed geometrically. This is done by computing the average sample for each of the two training Gaussian distributed data sets under consideration. Next the midpoint of the two average points is found. The classification boundary is drawn with at a $\pi/2$ angle to the line connecting the two average points. The result is shown in figure~\ref{fig:initGauss} on the left. A couple of points are misclassified by this approach, which given the statistical nature of the problem must always be allowed. In this case this method produces a decent classifier, this must not be true for other distributions though. If for example many additional samples distributed as shown in figure~\ref{fig:initGauss} on the right are added the method breaks down. This happened because the new samples moved the average and with it the decision boundary. In this new situation a large portion of the blue set is misclassified. 
Points far from the decision boundary should not influence the classifier like this. This does not happen to this extend if an optimal margin is sought which maximizes the distance of the classifier to each data set if separable or the best possible separation in terms of classification if the data set is not. Figure~\ref{fig:initGaussSvm} illustrates this.
\begin{figure}
\input{../src/tikz/initialGauss.tex}
\input{../src/tikz/initialGaussMod.tex}
\caption{Geometrical construction of a linear classification line using average values.}
\label{fig:initGauss}
\end{figure}
\begin{figure}
\input{../src/tikz/initialGaussSvm.tex}
\input{../src/tikz/initialGaussModSvm.tex}
\caption{Geometrical construction of a linear classification line using average values.}
\label{fig:initGaussSvm}
\end{figure}

\section{Vapnik Support Vector machines}
At the hart of vapnik's theory is the optimal hyperplane algorithm. In the linear the hyperplane condition is given as\footnote{Support-Vector Networks
CORINNA CORTES VLADIMIR VAPNIK,Machine Leaming, 20, (1995) page 291 or Least Squares Support Vector Machines, Suykens et al., page 31.}
\begin{equation}
y_k [\mathbf{w}^T \mathbf{x}_k + b] \geq 1, \;\; k = 1,\dots,N
\end{equation}
With $x_k$ being the input data points and $y_k$ the desired output. Training the machine means finding the high dimensional hyperplane normal vector $w$ and the bias scalar $b$. Normal means here that the dot product with any vector lying in the plane must be zero. Technically a plane is defined by $\mathbf{w}^T(\mathbf{x} - \mathbf{p}) = 0$ but its only interesting here to evaluate the classifier so $b = \mathbf{p}^T \mathbf{w}$ can be used instead and the displacement within the feature space remains unknown.
The next step is to formulate the optimization problem
\begin{equation}
\min_{w,b} \frac{1}{2} \mathbf{w}^T \mathbf{w} \;\text{ such that }\; y_k [\mathbf{w}^T \mathbf{x}_k + b] \geq 1.
\end{equation}
Formulating the Lagrange dual, and taking the gradient of $\mathcal{L}(\mathbf{w},b;\alpha)$ with respect to $(\mathbf{w},b)$ leads to a problem in $\alpha$. The lagrange multipliers are called $\alpha$, but in this context they will be called support vectors of the solution. From an optimization point of view, the support vectors are Lagrange multipliers with active set indices. A problem reformulation as
\begin{equation}
y_k [\mathbf{w}^T \phi(\mathbf{x}_k) + b] \geq 1
\end{equation}
allows for different kernel options. The classifier is given by
\begin{equation}
y(\mathbf{x}) = \text{sign}(\sum^N_{k=1} \alpha_k y_k K(\mathbf{x},\mathbf{x}_k) + b).
\end{equation}
The following three kernels will be considered here\footnote{Least Squares Support Vector Machines, Suykens et al., page 43.}
\begin{align}
K(\mathbf{x},\mathbf{x}_k)  &= \mathbf{x}_k^T \mathbf{x} &\text{  (linear SVM)}, \\
K(\mathbf{x}, \mathbf{x}_k) &= \exp (-\| \mathbf{x} - \mathbf{x}_k \|^2_2 / \sigma^2) &\text{  (RBF Kernel)}.
\end{align}

\begin{figure}
\centering
\includegraphics[width=0.33\linewidth]{../src/figure/svmjsLinSep}
\includegraphics[width=0.33\linewidth]{../src/figure/svmjsLinRBF}
\caption{Support vector machine classification of an almost linearly separable problem using a linear and an radial basis function kernel}
\label{fig:svmjsLinRBF}
\end{figure}

If the problem is not separable, it means that classification cannot be done without error. In the underlying optimization problem slack variables have to be included in the formulation:
\begin{equation}
y_k [\mathbf{w}^T \mathbf{x}_k + b] \geq 1 - \xi_k
\end{equation}
Which leads to the optimization problem:
\begin{align}
\min_{w,b,\xi} J_p(w,\xi) + c \sum_{k = 1}^N \xi_k \\
\text{such that }   y_k [\mathbf{w}^T \mathbf{x}_k + b] \geq 1 - \xi_k, \;\; k = 1,\dots,N \\
\xi_k \geq 0, \;\;\; k = 1,\dots,N 
\end{align}
Where $c$ is the positive real miss-classification penalty constant. High values of $c$ make erroneous classification more costly in terms of the merit function.  Low values make those cheaper. Using the \texttt{svmjs}\footnote{\url{http://cs.stanford.edu/people/karpathy/svmjs/demo/}} package the result of using different values of $c$ is illustrated in figure~\ref{fig:c}. Choosing the right value for $c$ is a balancing act. It is important to choose $c$ not too small. As very small values will result in underfitting the problem. The resulting classifier will ignore important features of the problem, as illustrated on the right in figure~\ref{fig:c}. On the other hand however $c$ must also not be too large. If the penalty on incorrect classification is too large the support machine will start to memorize nosy details, as illustrated in figure~\ref{fig:c} on the left. A good choice like the one shown in the middle captures the key points while not falling prey to the noise, while using just as many support vectors as necessary. 
\begin{figure}
\centering
\includegraphics[width=0.3\linewidth]{../src/figure/overfit}
\includegraphics[width=0.3\linewidth]{../src/figure/good}
\includegraphics[width=0.3\linewidth]{../src/figure/tooLow}
\caption{The effect of a large miss-classification penalty constant $c$ (left), a good choice for $c$ (middle) and a too small penalty value (right).}
\label{fig:c}
\end{figure}
The effect of changing the kernel density $\sigma$ is explored in figure $\ref{fig:sigma}$. In this case the classification error increase significantly for very small or very large $\sigma$ values. 
\begin{figure}
\centering
\includegraphics[width=0.3\linewidth]{../src/figure/sigmaTooSml}
\includegraphics[width=0.3\linewidth]{../src/figure/sigmaGood}
\includegraphics[width=0.3\linewidth]{../src/figure/sigmaTooLarge}
\caption{Using a  too small (left), a nicely chosen (middle) and too large (right) radial basis function parameter $\sigma$. }
\label{fig:sigma}
\end{figure}
Like choosing a good $c$ value, when picking $\sigma$ under and over-fitting considerations are important. Choosing the kernel too small as shown in figure~\ref{fig:sigma} on the left will result in over-fitting. Even tough all points are classified correctly the model misses the general overall geometry of the input data points completely. If $\sigma$ is too large under-fitting is observed in this case. Results are better in comparison to the very small $\sigma$-value but one would expect to see results of similar quality from a simple linear kernel. A good choice such as the one in figure~\ref{fig:sigma} in the middle captures the big picture while not using excessive amounts of support vectors.

\begin{figure}
\centering
\includegraphics[width=0.33\linewidth]{../src/figure/largeRBF}
\includegraphics[width=0.33\linewidth]{../src/figure/twoNewPoints}
\caption{The effect of two additional points on the boundary}
\label{fig:twoNewPoints}
\end{figure}
Figure~\ref{fig:twoNewPoints} illustrates the effect new data points have on the decision boundary. In the right plot two new red points have been added inside of the formerly entirely green region. The decision boundary changes if no additional miss-classification follows from this change. For the new red point close to the green boundary this cannot be done without miss-classifying the green points at the boundary, which therefore has to remain unchanged. 

\section{Least Squares Support Vector machines}